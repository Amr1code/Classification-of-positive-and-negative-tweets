{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPbXjGDraKPSCs0L0g1/Umz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import nltk\n","from nltk.corpus import twitter_samples\n","\n","nltk.download('twitter_samples')\n","\n","all_positive_tweets=twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets=twitter_samples.strings('negative_tweets.json')\n","print('Number of positive tweets: ',len(all_positive_tweets))\n","print('Number of negative tweets: ',len(all_negative_tweets))"],"metadata":{"id":"De37qOStLEmB","executionInfo":{"status":"ok","timestamp":1732311037176,"user_tz":-60,"elapsed":8751,"user":{"displayName":"Amr Rraci","userId":"06419097522947931800"}},"outputId":"eff5fe34-b95b-4627-82d0-4a97626e7f81","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Number of positive tweets:  5000\n","Number of negative tweets:  5000\n"]}]},{"cell_type":"code","source":["import nltk\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","nltk.download('stopwords')\n","#\n","nltk.download('punkt_tab')\n","#\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","# Data cleaning\n","def cleaned_tweet_with_lemmatization(tweet):\n","    # Remove URLs\n","    tweet = re.sub(r'http\\S+', '', tweet)\n","    # Remove punctuation\n","    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n","    # Convert to lowercase\n","    tweet = tweet.lower()\n","\n","    # Tokenize the tweet (now just tokenizing for lemmatization, no stemming)\n","    tokens = nltk.word_tokenize(tweet)\n","\n","    # Remove stopwords\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    # Perform lemmatization\n","    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","    # Join the lemmatized tokens back into a single string\n","    return ' '.join(lemmatized_tokens)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x8Hs7P3SZ9vC","executionInfo":{"status":"ok","timestamp":1732311041174,"user_tz":-60,"elapsed":784,"user":{"displayName":"Amr Rraci","userId":"06419097522947931800"}},"outputId":"09b18f98-3e3c-447a-d1c4-b12123aac509"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"uCE3TQ8VkRdO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cleaned_positive_tweets = [cleaned_tweet_with_lemmatization(tweet) for tweet in all_positive_tweets]\n","cleaned_negative_tweets = [cleaned_tweet_with_lemmatization(tweet) for tweet in all_negative_tweets]"],"metadata":{"id":"7jlMM576aDYl","executionInfo":{"status":"ok","timestamp":1732311054162,"user_tz":-60,"elapsed":9051,"user":{"displayName":"Amr Rraci","userId":"06419097522947931800"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Combining the tweets\n","tweets = all_positive_tweets + all_negative_tweets\n","# Creating labels for positive 1 and for negatives 0\n","labels = [1] * len(cleaned_positive_tweets) + [0] * len(cleaned_negative_tweets)\n","# Combining the cleaned tweets\n","cleaned_tweets = [cleaned_tweet_with_lemmatization(tweet) for tweet in tweets]\n","\n"],"metadata":{"id":"nTB-587_ab-_","executionInfo":{"status":"ok","timestamp":1732311058871,"user_tz":-60,"elapsed":3518,"user":{"displayName":"Amr Rraci","userId":"06419097522947931800"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","# Now we apply Tfi-Idf\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n","X = tfidf_vectorizer.fit_transform(cleaned_tweets)\n","print(\"TF-IDF Shape:\", X.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NGBLWQMZksd6","executionInfo":{"status":"ok","timestamp":1732311061875,"user_tz":-60,"elapsed":230,"user":{"displayName":"Amr Rraci","userId":"06419097522947931800"}},"outputId":"50d4d27b-140a-4554-8801-03d2163146c3"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["TF-IDF Shape: (10000, 5000)\n"]}]},{"cell_type":"code","source":["# Now we split the data in train set and test set\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)"],"metadata":{"id":"zcY6gQyik7tL","executionInfo":{"status":"ok","timestamp":1732311066901,"user_tz":-60,"elapsed":255,"user":{"displayName":"Amr Rraci","userId":"06419097522947931800"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","\n","svm_model = SVC(kernel='linear', random_state=42)\n","\n","\n","svm_model.fit(X_train, y_train)\n","\n","y_train_pred = svm_model.predict(X_train)\n","y_test_pred = svm_model.predict(X_test)\n","\n","# Training accuracy\n","train_accuracy = accuracy_score(y_train, y_train_pred)\n","print(f\"Training Accuracy: {train_accuracy:.2f}\")\n","\n","# Testing accuracy\n","test_accuracy = accuracy_score(y_test, y_test_pred)\n","print(f\"Testing Accuracy: {test_accuracy:.2f}\")\n","\n","# Detailed classification report for training data\n","print(\"\\nTraining Data Metrics:\")\n","print(classification_report(y_train, y_train_pred))\n","\n","# Detailed classification report for testing data\n","print(\"\\nTesting Data Metrics:\")\n","print(classification_report(y_test, y_test_pred))\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JbG09wQ2lURp","executionInfo":{"status":"ok","timestamp":1732311077040,"user_tz":-60,"elapsed":7454,"user":{"displayName":"Amr Rraci","userId":"06419097522947931800"}},"outputId":"fc6b2e06-6e44-4508-e25e-f3c7b5382268"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy: 0.89\n","Testing Accuracy: 0.76\n","\n","Training Data Metrics:\n","              precision    recall  f1-score   support\n","\n","           0       0.87      0.91      0.89      4012\n","           1       0.91      0.86      0.88      3988\n","\n","    accuracy                           0.89      8000\n","   macro avg       0.89      0.89      0.89      8000\n","weighted avg       0.89      0.89      0.89      8000\n","\n","\n","Testing Data Metrics:\n","              precision    recall  f1-score   support\n","\n","           0       0.74      0.79      0.76       988\n","           1       0.78      0.73      0.75      1012\n","\n","    accuracy                           0.76      2000\n","   macro avg       0.76      0.76      0.76      2000\n","weighted avg       0.76      0.76      0.76      2000\n","\n"]}]}]}